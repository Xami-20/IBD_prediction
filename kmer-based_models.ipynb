{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Xami-20/IBD_prediction/blob/main/kmer-based_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxXiTmmveppT"
   },
   "source": [
    "# **Packages**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Installing Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install numpy\n",
    "!python -m pip install pandas\n",
    "!python -m pip install sklearn\n",
    "!python -m pip install matplotlib\n",
    "!python -m pip install seaborn\n",
    "!python -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lun5sarpenx1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Python statistical data visualization library based on matplotlib\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.utils import resample # BootStrapping\n",
    "from sklearn.model_selection import train_test_split # Splitting the dataset into 70% Training set and 30% Testing set\n",
    "from sklearn.preprocessing import MinMaxScaler # Feature Scaling\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as RF # Importing random forest classifier from sklearn\n",
    "from sklearn.svm import SVC # Using SVC method of svm class to use linear Kernel Support Vector Machine Algorithm\n",
    "\n",
    "from sklearn.metrics import classification_report # Creating Classification Report\n",
    "from sklearn.metrics import confusion_matrix # Creating Confusion Matrix \n",
    "from sklearn.metrics import plot_confusion_matrix # plotting Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Getting Data Details**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_details(file): # file name = \"[4-25]-mers_decimal_PRJEB13679.csv\"\n",
    "    file = \"./kmers/labeled/\" + file    \n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # droping the sample name \n",
    "    df = df.drop(columns=[\"sample\"])\n",
    "    \n",
    "    # Print The number of rows and columns in the data set\n",
    "    print(df.shape)\n",
    "\n",
    "    # Print The new number of rows and columns in the data set\n",
    "    # print(df.shape)\n",
    "    \n",
    "#     # Visualize diagnosis counts\n",
    "#     plt.figure(figsize = (50,50))\n",
    "#     sns_plot = sns.countplot(df['diagnosis'],label=\"Count\")\n",
    "#     plt.rcParams.update({'font.size': 150})\n",
    "#     fig = sns_plot.get_figure()\n",
    "#     fig.savefig(\"Diagnosis.png\")\n",
    "\n",
    "    # Look at the data types to see which columns need to be transformed / encoded to a number\n",
    "    print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(file,n=None):\n",
    "\n",
    "    df = pd.read_csv(file)\n",
    "    df = df.drop(columns=[\"sample\"])\n",
    "    # df.iloc[:,0]\n",
    "    df.pivot_table(columns=['diagnosis'], aggfunc='size')\n",
    "    \n",
    "    # removing other IBDs diagnosis since the number of availbe samples is a limited representation\n",
    "    # CD    731\n",
    "    # IC     73\n",
    "    # UC    219\n",
    "    # no    336\n",
    "    # df = df.loc[(df[\"diagnosis\"] == \"CD\") | (df[\"diagnosis\"] == \"no\")]\n",
    "    \n",
    "    if type(n)!=type(None):\n",
    "        # BootStrapping resampling the data \n",
    "        from sklearn.utils import resample\n",
    "        df = resample(df, random_state=n, n_samples=n, replace=True)\n",
    "    \n",
    "#     # Change all diagnoses col to numical representation\n",
    "#     from sklearn.preprocessing import LabelEncoder\n",
    "#     labelencoder_Y = LabelEncoder()\n",
    "#     df.iloc[:,0] = labelencoder_Y.fit_transform(df.iloc[:,0].values)\n",
    "#     # print(df.iloc[:,0])\n",
    "    \n",
    "    # Split the data into independent 'X' and dependent 'Y' variables\n",
    "    X = df.iloc[:, 1:].values \n",
    "    Y = df.iloc[:, 0].values # Get the target variable 'diagnosis' located at index=1\n",
    "\n",
    "    \n",
    "    # Splitting the dataset into 70% Training set and 30% Testing set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train_data, X_test_data, Y_train_data, Y_test_data = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "    # Scale the data to bring all features to the same level of magnitude\n",
    "\n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_data = scaler.fit_transform(X_train_data)\n",
    "    X_test_data = scaler.transform(X_test_data)\n",
    "    res = [X_train_data, X_test_data, Y_train_data, Y_test_data]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = sorted(os.listdir(\"./kmers/labeled/\"))\n",
    "file = \"./kmers/labeled/\" + files[0]\n",
    "df = pd.read_csv(file)\n",
    "df.pivot_table(columns=['diagnosis'], aggfunc='size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest_classifier(X_train_data, X_test_data, Y_train_data, Y_test_data,trees = 100):\n",
    "    # Importing random forest classifier from sklearn\n",
    "    from sklearn.ensemble import RandomForestClassifier as RF\n",
    "    # Creating the Rf Classifier\n",
    "    RF_classifier = RF(n_estimators = trees, criterion = \"entropy\", random_state = 0)\n",
    "\n",
    "    # Fit the model using the training dataset\n",
    "    RF_classifier.fit(X_train_data, Y_train_data)\n",
    "\n",
    "    # Predicting the results of the Testing dataset\n",
    "    prediction = RF_classifier.predict(X_test_data)\n",
    "\n",
    "    trainscore = RF_classifier.score(X_train_data, Y_train_data)*100\n",
    "    testscore = RF_classifier.score(X_test_data, Y_test_data)*100\n",
    "#     print('\\nTrees: {}\\nRandom Forest Classifier Training Accuracy: {:.2f} %'.format(trees,trainscore))\n",
    "#     print('Random Forest Classifier Testing Accuracy: {:.2f} %'.format(testscore))\n",
    "\n",
    "    target_names = ['CD','IC','UC','no']\n",
    "    \n",
    "    from sklearn.metrics import classification_report\n",
    "    report = classification_report(Y_test_data, prediction,labels=target_names)\n",
    "#     print(report)\n",
    "    \n",
    "    # Creating Confusion Matrix \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(Y_test_data, prediction,labels=target_names)\n",
    "#     print(cm)\n",
    "    \n",
    "    # plotting Confusion matrix\n",
    "    from sklearn.metrics import plot_confusion_matrix\n",
    "#     plot_confusion_matrix(RF_classifier,X_test_data, Y_test_data)\n",
    "    \n",
    "    model = 'RF_{}-Trees'.format(trees)\n",
    "    return {'Model': model,'Train Score':trainscore, 'Test Score': testscore,\n",
    "            'Classification Report':report,'Confusion Matrix':cm}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_linear_model(X_train_data, X_test_data, Y_train_data, Y_test_data): \n",
    "    # Using SVC method of svm class to use linear Kernel Support Vector Machine Algorithm\n",
    "    from sklearn.svm import SVC\n",
    "    svc_lin = SVC(kernel = 'linear', random_state = 0)\n",
    "    svc_lin.fit(X_train_data, Y_train_data)\n",
    "    \n",
    "    # Predicting the results of the Testing dataset\n",
    "    prediction = svc_lin.predict(X_test_data)\n",
    "    \n",
    "    trainscore = svc_lin.score(X_train_data, Y_train_data)*100\n",
    "    testscore = svc_lin.score(X_test_data, Y_test_data)*100\n",
    "#     print('Support Vector Machine (Linear Classifier) Training Accuracy:', \"{:.2f} %\".format(trainscore))\n",
    "#     print('Support Vector Machine (Linear Classifier) Testing Accuracy:', \"{:.2f} %\".format(testscore))\n",
    "    \n",
    "    target_names = ['CD','IC','UC','no']\n",
    "    \n",
    "    from sklearn.metrics import classification_report\n",
    "    report = classification_report(Y_test_data, prediction,labels=target_names)\n",
    "#     print(report)\n",
    "    \n",
    "    # Creating Confusion Matrix \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(Y_test_data, prediction,labels=target_names)\n",
    "#     print(cm)\n",
    "    \n",
    "    # Plotting Confusion matrix\n",
    "    from sklearn.metrics import plot_confusion_matrix\n",
    "#     plot_confusion_matrix(svc_lin,X_test_data, Y_test_data)\n",
    "        \n",
    "    return {'Model': 'SVM (Linear)','Train Score':trainscore, 'Test Score': testscore,\n",
    "            'Classification Report':report,'Confusion Matrix':cm}\n",
    "\n",
    "def svm_poly_model(X_train_data, X_test_data, Y_train_data, Y_test_data):\n",
    "    # Using SVC method of svm class to use poly Kernel SVM Algorithm\n",
    "    from sklearn.svm import SVC\n",
    "    svc_poly = SVC(kernel = 'poly', random_state = 0)\n",
    "    svc_poly.fit(X_train_data, Y_train_data)\n",
    "        \n",
    "    # Predicting the results of the Testing dataset\n",
    "    prediction = svc_poly.predict(X_test_data)\n",
    "    \n",
    "    trainscore = svc_poly.score(X_train_data, Y_train_data)*100\n",
    "    testscore = svc_poly.score(X_test_data, Y_test_data)*100\n",
    "#     print('Support Vector Machine (Poly Classifier) Training Accuracy:', \"{:.2f} %\".format(trainscore))\n",
    "#     print('Support Vector Machine (Poly Classifier) Testing Accuracy:', \"{:.2f} %\".format(testscore))\n",
    "    \n",
    "    target_names = ['CD','IC','UC','no']\n",
    "    \n",
    "    from sklearn.metrics import classification_report\n",
    "    report = classification_report(Y_test_data, prediction,labels=target_names)\n",
    "#     print(report)\n",
    "    \n",
    "    # Creating Confusion Matrix \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(Y_test_data, prediction,labels=target_names)\n",
    "#     print(cm)\n",
    "    \n",
    "    # Plotting Confusion matrix\n",
    "    from sklearn.metrics import plot_confusion_matrix\n",
    "#     plot_confusion_matrix(svc_poly,X_test_data, Y_test_data)\n",
    "        \n",
    "    return {'Model': 'SVM (Poly)','Train Score':trainscore, 'Test Score': testscore,\n",
    "            'Classification Report':report,'Confusion Matrix':cm}\n",
    "\n",
    "def svm_rbf_model(X_train_data, X_test_data, Y_train_data, Y_test_data):\n",
    "    # Using SVC method of svm class to use rbf Kernel SVM Algorithm\n",
    "    from sklearn.svm import SVC\n",
    "    svc_rbf = SVC(kernel = 'rbf', random_state = 0)\n",
    "    svc_rbf.fit(X_train_data, Y_train_data)\n",
    "        \n",
    "    # Predicting the results of the Testing dataset\n",
    "    prediction = svc_rbf.predict(X_test_data)\n",
    "    \n",
    "    trainscore = svc_rbf.score(X_train_data, Y_train_data)*100\n",
    "    testscore = svc_rbf.score(X_test_data, Y_test_data)*100\n",
    "#     print('Support Vector Machine (RBF Classifier) Training Accuracy:', \"{:.2f} %\".format(trainscore))\n",
    "#     print('Support Vector Machine (RBF Classifier) Testing Accuracy:', \"{:.2f} %\".format(testscore))\n",
    "    \n",
    "    target_names = ['CD','IC','UC','no']\n",
    "    \n",
    "    from sklearn.metrics import classification_report\n",
    "    report = classification_report(Y_test_data, prediction,labels=target_names)\n",
    "#     print(report)\n",
    "    \n",
    "    # Creating Confusion Matrix \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(Y_test_data, prediction,labels=target_names)\n",
    "#     print(cm)\n",
    "    \n",
    "    # Plotting Confusion matrix\n",
    "    from sklearn.metrics import plot_confusion_matrix\n",
    "#     plot_confusion_matrix(svc_rbf,X_test_data, Y_test_data)\n",
    "    \n",
    "    return {'Model': 'SVM (RBF)','Train Score':trainscore, 'Test Score': testscore,\n",
    "            'Classification Report':report,'Confusion Matrix':cm}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_files = sorted(os.listdir(\"./kmers/labeled/\"))\n",
    "for i in data_files:\n",
    "    file = \"./kmers/labeled/\" + i\n",
    "    print(i)\n",
    "    n_samples = [100, 200, 500, 1000, 2000, 5000, 10000, None]\n",
    "    for n in n_samples:\n",
    "#         print(\"Number of Samples:\\t{}\".format(n))\n",
    "        data = data_preparation(file,n)\n",
    "        \n",
    "        svm_lin = svm_linear_model(data[0],data[1],data[2],data[3])\n",
    "#         print(\"\\n\")\n",
    "        poly = svm_poly_model(data[0],data[1],data[2],data[3])\n",
    "#         print(\"\\n\")\n",
    "        rbf = svm_rbf_model(data[0],data[1],data[2],data[3])\n",
    "        \n",
    "        rf50 = RandomForest_classifier(data[0],data[1],data[2],data[3],trees=50)\n",
    "        rf100 = RandomForest_classifier(data[0],data[1],data[2],data[3],trees=100)\n",
    "        rf150 = RandomForest_classifier(data[0],data[1],data[2],data[3],trees=150)\n",
    "#         print(\"\\n\")\n",
    "        if type(n)!=type(None):\n",
    "            df_name = './Results/' + i[:i.find('_decimal')] + '_' + str(n) + '-Resampling_Models\\'Results'  + '.csv'\n",
    "        else:\n",
    "            df_name = './Results/' + i[:i.find('_decimal')] + '_All_Samples_Models\\'Results'  + '.csv'\n",
    "        df = pd.DataFrame([svm_lin,poly,rbf,rf50,rf100,rf150])\n",
    "        df.to_csv(df_name,index=False)\n",
    "    print(\"\\nNext,\\n\")\n",
    "# print(x)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
